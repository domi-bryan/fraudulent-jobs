{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraudulent Job Posting Detectors\n",
    "\n",
    "The following notebook is aimed to create and compare classifier tools to detect fraudulent job postings using only the title, description, and requirement.\n",
    "\n",
    "## Datasets used\n",
    "\n",
    "Two datasets are used from Kaggle:\n",
    "<ol>\n",
    "    <li> Fake Postings.csv (https://www.kaggle.com/datasets/srisaisuhassanisetty/fake-job-postings)\n",
    "    <li> data job posts.csv (https://www.kaggle.com/datasets/madhab/jobposts)\n",
    "    <li> job_train.csv (https://www.kaggle.com/datasets/prxshetty/fake-real-job-listings-dataset)\n",
    "    <li> (https://www.kaggle.com/datasets/shivamb/real-or-fake-fake-jobposting-prediction)\n",
    "</ol>\n",
    "\n",
    "Dataset #1 has 10,000 rows, all of which are fraudulent.\n",
    "\n",
    "Dataset #2 has 19,000 rows, all of which are assumed to be legitimate with some NA values. Those with NA in the title/description/requirements are dropped. This resulted in 13,124 rows remaining.\n",
    "\n",
    "Dataset #3 has 8,940 rows, some of which are legitimate, and some fraudulent.\n",
    "\n",
    "Dataset #4 has 17,880 rows, some of which are legitimate, and some fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the datasets\n",
    "df1 = pd.read_csv('Fake Postings.csv')\n",
    "df2 = pd.read_csv('data job posts.csv')\n",
    "df3 = pd.read_csv('job_train.csv')\n",
    "df4 = pd.read_csv('fake_job_postings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['text'] = df1['title'].fillna('') + ' ' + df1['description'].fillna('') + ' ' + df1['requirements'].fillna('')\n",
    "df1['fraudulent'] = 1\n",
    "\n",
    "df2.dropna(subset=['Title', 'JobDescription', 'JobRequirment', 'RequiredQual'], inplace=True)\n",
    "df2['text'] = (\n",
    "    df2['Title'].fillna('') + ' ' + df2['JobDescription'].fillna('') + ' ' +\n",
    "    df2['JobRequirment'].fillna('') + ' ' + df2['RequiredQual'].fillna('')\n",
    ")\n",
    "df2['fraudulent'] = 0\n",
    "\n",
    "df3['title'] = df3['title'].apply(lambda x: x.split('-')[0].strip())\n",
    "df3['text'] = df3['title'].fillna('') + ' ' + df3['description'].fillna('') + ' ' + df3['requirements'].fillna('')\n",
    "\n",
    "df4['title'] = df4['title'].apply(lambda x: x.split('-')[0].strip())\n",
    "df4['text'] = df4['title'].fillna('') + ' ' + df4['description'].fillna('') + ' ' + df4['requirements'].fillna('')\n",
    "\n",
    "train_df = pd.concat([df1[['text', 'fraudulent']], df2[['text', 'fraudulent']], df3[['text', 'fraudulent']], df4[['text', 'fraudulent']]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1) + len(df3[df3['fraudulent'] == 1]) + len(df4[df4['fraudulent'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1) + len(df3[df3['fraudulent'] == 0]) + len(df4[df4['fraudulent'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train_data['text']).toarray()\n",
    "y_train = train_data['fraudulent']\n",
    "\n",
    "X_test = vectorizer.transform(test_data['text']).toarray()\n",
    "y_test = test_data['fraudulent']\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_df = pd.DataFrame(columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 8)\n",
    "knn.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraudulent', 'Fraudulent'], yticklabels=['Not Fraudulent', 'Fraudulent'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Create a new DataFrame with the metrics\n",
    "\n",
    "if 'K-Nearest Neighbours' not in all_stats_df['Classifier'].values:\n",
    "    new_stats_df = pd.DataFrame([{\n",
    "        'Classifier': 'K-Nearest Neighbours',\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }])\n",
    "\n",
    "    all_stats_df = pd.concat([all_stats_df, new_stats_df], ignore_index=True)\n",
    "\n",
    "all_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "Accuracy: 83.4%\n",
    "\n",
    "Precision: 58.4%\n",
    "\n",
    "Recall: 99.2%\n",
    "\n",
    "F1-Score: 73.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_log_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_log_reg = confusion_matrix(y_test, y_pred_log_reg)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_log_reg, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraudulent', 'Fraudulent'], yticklabels=['Not Fraudulent', 'Fraudulent'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix: Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "precision = precision_score(y_test, y_pred_log_reg)\n",
    "recall = recall_score(y_test, y_pred_log_reg)\n",
    "f1 = f1_score(y_test, y_pred_log_reg)\n",
    "\n",
    "# Create a new DataFrame with the metrics\n",
    "\n",
    "if 'Logistics Regression' not in all_stats_df['Classifier'].values:\n",
    "    new_stats_df = pd.DataFrame([{\n",
    "        'Classifier': 'Logistics Regression',\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }])\n",
    "\n",
    "    all_stats_df = pd.concat([all_stats_df, new_stats_df], ignore_index=True)\n",
    "\n",
    "all_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "Accuracy: 98.6%\n",
    "\n",
    "Precision: 98.8%\n",
    "\n",
    "Recall: 95.3%\n",
    "\n",
    "F1-Score: 97.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine + PCA\n",
    "\n",
    "From Trial and Error, SVM alone takes a long time to run. Hence, PCA will also be used to reduce complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_resampled)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "sum(list(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(random_state=42, kernel='linear')\n",
    "svm.fit(X_train_pca, y_train_resampled)\n",
    "y_pred_svm = svm.predict(X_test_pca)\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_svm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraudulent', 'Fraudulent'], yticklabels=['Not Fraudulent', 'Fraudulent'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix: SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "precision = precision_score(y_test, y_pred_svm)\n",
    "recall = recall_score(y_test, y_pred_svm)\n",
    "f1 = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "# Create a new DataFrame with the metrics\n",
    "\n",
    "if 'Support Vector Machine' not in all_stats_df['Classifier'].values:\n",
    "    new_stats_df = pd.DataFrame([{\n",
    "        'Classifier': 'Support Vector Machine',\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }])\n",
    "\n",
    "    all_stats_df = pd.concat([all_stats_df, new_stats_df], ignore_index=True)\n",
    "\n",
    "all_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "Accuracy: 98.0%\n",
    "\n",
    "Precision: 97.7%\n",
    "\n",
    "Recall: 93.4%\n",
    "\n",
    "F1-Score: 95.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "To be continued..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_tree = confusion_matrix(y_test, y_pred_tree)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_tree, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraudulent', 'Fraudulent'], yticklabels=['Not Fraudulent', 'Fraudulent'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix: Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "precision = precision_score(y_test, y_pred_svm)\n",
    "recall = recall_score(y_test, y_pred_svm)\n",
    "f1 = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "# Create a new DataFrame with the metrics\n",
    "\n",
    "if 'Decision Tree' not in all_stats_df['Classifier'].values:\n",
    "    new_stats_df = pd.DataFrame([{\n",
    "        'Classifier': 'Decision Tree',\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }])\n",
    "\n",
    "    all_stats_df = pd.concat([all_stats_df, new_stats_df], ignore_index=True)\n",
    "\n",
    "all_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "\n",
    "Accuracy: --%\n",
    "\n",
    "Precision: --%\n",
    "\n",
    "Recall: --%\n",
    "\n",
    "F1-Score: --%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base classifiers\n",
    "base_classifiers = [\n",
    "    ('svm', SVC(kernel='linear', random_state=42)),\n",
    "    ('log_reg', LogisticRegression(random_state=42))\n",
    "]\n",
    "\n",
    "# Define the stacking classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_classifiers,\n",
    "    final_estimator=DecisionTreeClassifier(random_state=42),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Fit the stacking classifier on the resampled training data\n",
    "stacking_clf.fit(X_train_pca, y_train_resampled)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_stacking = stacking_clf.predict(X_test_pca)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Stacking Classifier Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_stacking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_stacking = confusion_matrix(y_test, y_pred_stacking)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_stacking, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraudulent', 'Fraudulent'], yticklabels=['Not Fraudulent', 'Fraudulent'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix: Stacking Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_stacking)\n",
    "precision = precision_score(y_test, y_pred_stacking)\n",
    "recall = recall_score(y_test, y_pred_stacking)\n",
    "f1 = f1_score(y_test, y_pred_stacking)\n",
    "\n",
    "# Create a new DataFrame with the metrics\n",
    "if 'Stacking Classifier' not in all_stats_df['Classifier'].values:\n",
    "    new_stats_df = pd.DataFrame([{\n",
    "        'Classifier': 'Stacking Classifier',\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }])\n",
    "\n",
    "    all_stats_df = pd.concat([all_stats_df, new_stats_df], ignore_index=True)\n",
    "\n",
    "all_stats_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
